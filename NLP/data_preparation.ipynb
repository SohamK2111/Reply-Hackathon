{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SohamK2111/Reply-Hackathon/blob/main/NLP/data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo2yl5UKySU7"
      },
      "source": [
        "# Natural Langauge Processing\n",
        "In this notebook, we will be focusing on a standard way of processing the data to feed to a model such as GPT-3. For this we will be using various libraries (please bare in mind that this was attempted locall!! Helper and utility functions were used for loading aspects i.e. credentials and dataset downloads)\n",
        "\n",
        "The actual fine-tuning of a GPT-3 model can be found here: https://platform.openai.com/docs/guides/fine-tuning "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-VtcmxJySU-",
        "outputId": "828cb3a1-6ec7-431c-cd37-98d008219324"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading wordnet: <urlopen error [Errno 8] nodename\n",
            "[nltk_data]     nor servname provided, or not known>\n"
          ]
        }
      ],
      "source": [
        "# Import useful Libraries\n",
        "import credentials\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import openai\n",
        "import gensim\n",
        "import kaggle\n",
        "import string\n",
        "import nltk\n",
        "import json\n",
        "import os\n",
        "nltk.download('wordnet')\n",
        "stemmer = nltk.stem.SnowballStemmer('english')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efJJr5JkySVA"
      },
      "source": [
        "# Get credentials \n",
        "Locally attempted so please bear in mind!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxT4OPBzySVB"
      },
      "outputs": [],
      "source": [
        "openai.api_key = credentials.get_openai_api_key()\n",
        "kaggle_credentials = credentials.get_kaggle_creds('~/.kaggle/kaggle.json')\n",
        "os.environ[\"KAGGLE_USERNAME\"] = kaggle_credentials[\"username\"]\n",
        "os.environ[\"KAGGLE_KEY\"] = kaggle_credentials[\"key\"]\n",
        "zipped_path = \"datasets/zipped/\"\n",
        "extracted_path = \"datasets/extracted/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT4onhVqySVB"
      },
      "source": [
        "Download dataset(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIJxSGQZySVC"
      },
      "outputs": [],
      "source": [
        "# downloads the dataset into a zip file\n",
        "kaggle.api.dataset_download_files(\"patrickfleith/space-news-dataset\", path=zipped_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-Kpb4u8ySVC"
      },
      "outputs": [],
      "source": [
        "# File needs to be unzipped\n",
        "raw_zip = f\"{zipped_path}space-news-dataset.zip\"\n",
        "with zipfile.ZipFile(raw_zip, \"r\") as zip_file:\n",
        "    zip_file.extractall(extracted_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieJ__IBJySVC"
      },
      "source": [
        "Unzipping the file gives us a csv filled with tabular based data. We can take a look at this data using the Pandas Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZqatMbnySVD",
        "outputId": "6ebbb5e5-6512-4649-a37d-dd821d80e027"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>content</th>\n",
              "      <th>author</th>\n",
              "      <th>date</th>\n",
              "      <th>postexcerpt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Orion splashes down to end Artemis 1</td>\n",
              "      <td>https://spacenews.com/orion-splashes-down-to-e...</td>\n",
              "      <td>Updated at 5:45 p.m. Eastern after post-splash...</td>\n",
              "      <td>Jeff Foust</td>\n",
              "      <td>December 11, 2022</td>\n",
              "      <td>Fifty years to the day after the last Apollo m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Polaris Dawn crewed mission could suffer addit...</td>\n",
              "      <td>https://spacenews.com/polaris-dawn-crewed-miss...</td>\n",
              "      <td>LAS VEGAS — A billionaire-backed private astro...</td>\n",
              "      <td>Jeff Foust</td>\n",
              "      <td>October 25, 2022</td>\n",
              "      <td>A billionaire-backed private astronaut mission...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DART on track for asteroid collision</td>\n",
              "      <td>https://spacenews.com/dart-on-track-for-astero...</td>\n",
              "      <td>WASHINGTON — A NASA spacecraft is on course to...</td>\n",
              "      <td>Jeff Foust</td>\n",
              "      <td>September 25, 2022</td>\n",
              "      <td>A NASA spacecraft is on course to deliberately...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>U.S. Space Command calls for investment in tec...</td>\n",
              "      <td>https://spacenews.com/u-s-space-command-calls-...</td>\n",
              "      <td>WASHINGTON — Lt. Gen. John Shaw, deputy comman...</td>\n",
              "      <td>Sandra Erwin</td>\n",
              "      <td>August 31, 2022</td>\n",
              "      <td>U.S. Space Command's Lt. Gen. John Shaw said '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SpaceX requests permission for direct-to-smart...</td>\n",
              "      <td>https://spacenews.com/spacex-requests-permissi...</td>\n",
              "      <td>TAMPA, Fla. — SpaceX could provide “full and c...</td>\n",
              "      <td>Jason Rainbow</td>\n",
              "      <td>December 8, 2022</td>\n",
              "      <td>SpaceX could provide “full and continuous” dir...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  \\\n",
              "0               Orion splashes down to end Artemis 1   \n",
              "1  Polaris Dawn crewed mission could suffer addit...   \n",
              "2               DART on track for asteroid collision   \n",
              "3  U.S. Space Command calls for investment in tec...   \n",
              "4  SpaceX requests permission for direct-to-smart...   \n",
              "\n",
              "                                                 url  \\\n",
              "0  https://spacenews.com/orion-splashes-down-to-e...   \n",
              "1  https://spacenews.com/polaris-dawn-crewed-miss...   \n",
              "2  https://spacenews.com/dart-on-track-for-astero...   \n",
              "3  https://spacenews.com/u-s-space-command-calls-...   \n",
              "4  https://spacenews.com/spacex-requests-permissi...   \n",
              "\n",
              "                                             content         author  \\\n",
              "0  Updated at 5:45 p.m. Eastern after post-splash...     Jeff Foust   \n",
              "1  LAS VEGAS — A billionaire-backed private astro...     Jeff Foust   \n",
              "2  WASHINGTON — A NASA spacecraft is on course to...     Jeff Foust   \n",
              "3  WASHINGTON — Lt. Gen. John Shaw, deputy comman...   Sandra Erwin   \n",
              "4  TAMPA, Fla. — SpaceX could provide “full and c...  Jason Rainbow   \n",
              "\n",
              "                 date                                        postexcerpt  \n",
              "0   December 11, 2022  Fifty years to the day after the last Apollo m...  \n",
              "1    October 25, 2022  A billionaire-backed private astronaut mission...  \n",
              "2  September 25, 2022  A NASA spacecraft is on course to deliberately...  \n",
              "3     August 31, 2022  U.S. Space Command's Lt. Gen. John Shaw said '...  \n",
              "4    December 8, 2022  SpaceX could provide “full and continuous” dir...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create a dataframe\n",
        "df = pd.read_csv(f\"{extracted_path}spacenews-december-2022.csv\")\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EacoGF_ySVD"
      },
      "source": [
        "Now we can see that there is some data which we can use. From the column names alone, we can understand that the columns: Title, content and postexerpt seem like they would contain appropriate textual data which we can use for our purposes. We can deepdive into these colums"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmSh6kkhySVE"
      },
      "outputs": [],
      "source": [
        "interesting_columns = [\"title\", \"content\", \"postexcerpt\"]\n",
        "interesting_df = df.loc[:, interesting_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mfe9t5rQySVE",
        "outputId": "ab150362-f3de-40e0-e51c-1fcba5b63d35"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>postexcerpt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Orion splashes down to end Artemis 1</td>\n",
              "      <td>Updated at 5:45 p.m. Eastern after post-splash...</td>\n",
              "      <td>Fifty years to the day after the last Apollo m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Polaris Dawn crewed mission could suffer addit...</td>\n",
              "      <td>LAS VEGAS — A billionaire-backed private astro...</td>\n",
              "      <td>A billionaire-backed private astronaut mission...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DART on track for asteroid collision</td>\n",
              "      <td>WASHINGTON — A NASA spacecraft is on course to...</td>\n",
              "      <td>A NASA spacecraft is on course to deliberately...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>U.S. Space Command calls for investment in tec...</td>\n",
              "      <td>WASHINGTON — Lt. Gen. John Shaw, deputy comman...</td>\n",
              "      <td>U.S. Space Command's Lt. Gen. John Shaw said '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SpaceX requests permission for direct-to-smart...</td>\n",
              "      <td>TAMPA, Fla. — SpaceX could provide “full and c...</td>\n",
              "      <td>SpaceX could provide “full and continuous” dir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18349</th>\n",
              "      <td>Kendall lays out Pentagon thinking on future s...</td>\n",
              "      <td>\\nFrank Kendall, the Pentagon’s top acquisitio...</td>\n",
              "      <td>Frank Kendall, the Pentagon’s top acquisition ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18350</th>\n",
              "      <td>A larger share of NOAA’s declining space budge...</td>\n",
              "      <td>Updated Feb. 10 at 10:18 p.m. Eastern The U.S....</td>\n",
              "      <td>The U.S. National Oceanic and Atmospheric Admi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18351</th>\n",
              "      <td>Think Tank Turns Its Attention To Mars As 2016...</td>\n",
              "      <td>WASHINGTON — As NASA develops a long-term stra...</td>\n",
              "      <td>As NASA develops a long-term strategy to suppo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18352</th>\n",
              "      <td>House Bill Leaves Last Three JPSS Satellites i...</td>\n",
              "      <td>WASHINGTON — A spending bill the House passed ...</td>\n",
              "      <td>A spending bill the House passed June 3 would ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18353</th>\n",
              "      <td>Championing a Climate Change for Commercial We...</td>\n",
              "      <td>U.S. Rep. Jim Bridenstine (R-Okla.) is a growi...</td>\n",
              "      <td>Rep. Jim Bridenstine, who emerged in 2014 as a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18354 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  \\\n",
              "0                   Orion splashes down to end Artemis 1   \n",
              "1      Polaris Dawn crewed mission could suffer addit...   \n",
              "2                   DART on track for asteroid collision   \n",
              "3      U.S. Space Command calls for investment in tec...   \n",
              "4      SpaceX requests permission for direct-to-smart...   \n",
              "...                                                  ...   \n",
              "18349  Kendall lays out Pentagon thinking on future s...   \n",
              "18350  A larger share of NOAA’s declining space budge...   \n",
              "18351  Think Tank Turns Its Attention To Mars As 2016...   \n",
              "18352  House Bill Leaves Last Three JPSS Satellites i...   \n",
              "18353  Championing a Climate Change for Commercial We...   \n",
              "\n",
              "                                                 content  \\\n",
              "0      Updated at 5:45 p.m. Eastern after post-splash...   \n",
              "1      LAS VEGAS — A billionaire-backed private astro...   \n",
              "2      WASHINGTON — A NASA spacecraft is on course to...   \n",
              "3      WASHINGTON — Lt. Gen. John Shaw, deputy comman...   \n",
              "4      TAMPA, Fla. — SpaceX could provide “full and c...   \n",
              "...                                                  ...   \n",
              "18349  \\nFrank Kendall, the Pentagon’s top acquisitio...   \n",
              "18350  Updated Feb. 10 at 10:18 p.m. Eastern The U.S....   \n",
              "18351  WASHINGTON — As NASA develops a long-term stra...   \n",
              "18352  WASHINGTON — A spending bill the House passed ...   \n",
              "18353  U.S. Rep. Jim Bridenstine (R-Okla.) is a growi...   \n",
              "\n",
              "                                             postexcerpt  \n",
              "0      Fifty years to the day after the last Apollo m...  \n",
              "1      A billionaire-backed private astronaut mission...  \n",
              "2      A NASA spacecraft is on course to deliberately...  \n",
              "3      U.S. Space Command's Lt. Gen. John Shaw said '...  \n",
              "4      SpaceX could provide “full and continuous” dir...  \n",
              "...                                                  ...  \n",
              "18349  Frank Kendall, the Pentagon’s top acquisition ...  \n",
              "18350  The U.S. National Oceanic and Atmospheric Admi...  \n",
              "18351  As NASA develops a long-term strategy to suppo...  \n",
              "18352  A spending bill the House passed June 3 would ...  \n",
              "18353  Rep. Jim Bridenstine, who emerged in 2014 as a...  \n",
              "\n",
              "[18354 rows x 3 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "interesting_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM5pjlEfySVE"
      },
      "source": [
        "# Cleaning the data\n",
        "To clean the data, we want to carry out text normalisation. For this, we will need to carry out some of the following operations:\n",
        "  - Convert to lowercase \n",
        "  - Remove punctuation \n",
        "  - Remove special characters \n",
        "  - Lemmatise and Stem words \n",
        "  - Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW25ZhESySVF"
      },
      "outputs": [],
      "source": [
        "interesting_df[\"title\"] = interesting_df[\"title\"].str.lower()\n",
        "interesting_df[\"content\"] = interesting_df[\"content\"].str.lower()\n",
        "interesting_df[\"postexcerpt\"] = interesting_df[\"postexcerpt\"].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ96LiG6ySVF",
        "outputId": "36a995d6-1ecd-4e8f-e53f-0b37a27c62ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/saifsaleem/opt/miniconda3/envs/hackcambridge/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # Helper method to remove punctuation from text\n",
        "    punctuation = string.punctuation\n",
        "    clean_text = ''.join([char for char in text if char not in punctuation])\n",
        "    clean_text = clean_text.encode('ascii', 'ignore')\n",
        "    return str(clean_text)\n",
        "\n",
        "def lemm_and_stem(text):\n",
        "    # Helper method to lemmatize and stem words\n",
        "    return stemmer.stem(lemmatizer.lemmatize(text, pos='v'))\n",
        "\n",
        "def preprocess_text(text, topic_list=True):\n",
        "    # Lemmatise, stem, stopword removal and tokenisation of text\n",
        "    if not topic_list:\n",
        "        clean_text_tokens = [lemm_and_stem(token) for token in nltk.word_tokenize(text) if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3]\n",
        "        clean_text = \" \".join(clean_text_tokens)\n",
        "        clean_text = remove_punctuation(clean_text)\n",
        "        return clean_text\n",
        "    # Otherwise we can create a list of words to use as a reference in building the attention mask\n",
        "    return [lemm_and_stem(token) for token in nltk.word_tokenize(text) if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3]\n",
        "\n",
        "# Pre-process titles to get keywords to apply attention masking to df\n",
        "title_df = pd.DataFrame(columns=[\"title\"], data=interesting_df[\"title\"].copy())\n",
        "postexcerpt_df = pd.DataFrame(columns=[\"postexcerpt\"], data=interesting_df[\"postexcerpt\"].copy())\n",
        "\n",
        "\"\"\"We want to understand if there are some interesting topics available to us. Normally, we can use LDAs for topic modelling,\n",
        "however, since this dataset is based on space, we will end up with one main topic. As an alternative, we can utilise, some \n",
        "simple pre-processing techniques such as the ones described above to come up with a list of topics which we can use.\n",
        "\n",
        "For this, we can extract such words from the title DataFrame and thepostexcerpt DataFrame\n",
        "\"\"\"\n",
        "title_df[\"title\"] = title_df[\"title\"].astype('str').apply(lambda text: preprocess_text(text, topic_list=True))\n",
        "postexcerpt_df[\"postexcerpt\"] = postexcerpt_df[\"postexcerpt\"].astype('str').apply(lambda text: preprocess_text(text, topic_list=True))\n",
        "\n",
        "interesting_df[\"title\"] = interesting_df[\"title\"].astype('str').apply(lambda text: preprocess_text(text, topic_list=False))\n",
        "interesting_df[\"content\"] = interesting_df[\"content\"].astype('str').apply(lambda text: preprocess_text(text, topic_list=False))\n",
        "interesting_df[\"postexcerpt\"] = interesting_df[\"postexcerpt\"].astype('str').apply(lambda text: preprocess_text(text, topic_list=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9BeSIA0ySVG"
      },
      "source": [
        "Now, we have DataFrames which contain some of the interesting topics, as well as our pre-processed data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwniNKg2ySVG",
        "outputId": "b1f581b2-ba31-4c24-b43e-33d22f002165"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "def concatenate_columns(row):\n",
        "    return row.title + row.postexcerpt\n",
        "\n",
        "def get_attention(row): \n",
        "    attention_mask = []\n",
        "    words = row[\"attention_words\"]\n",
        "    content = row[\"attention_content\"]\n",
        "    for word in nltk.word_tokenize(content): \n",
        "        if word in words: \n",
        "            attention_mask.append(1)\n",
        "        else:\n",
        "            attention_mask.append(0)\n",
        "    return attention_mask\n",
        "\n",
        "def tokenize(text, tokenizer=GPT2Tokenizer.from_pretrained(\"gpt2\")):\n",
        "    tokenised = tokenizer(text)\n",
        "    return tokenised[\"input_ids\"]\n",
        "\n",
        "# Join Title and PostExcerpt DataFrames in to one to start the creation of an attention mask. \n",
        "# We can effectively use this list of words to automate the creation of our attention mask!\n",
        "attention_df = pd.DataFrame(columns=[\"attention_words\"], data=title_df.join(postexcerpt_df, how='outer').apply(lambda row: concatenate_columns(row), axis=1))\n",
        "attention_df[\"attention_content\"] = interesting_df[\"content\"]\n",
        "attention_df[\"attention_mask\"] = attention_df.apply(lambda row: get_attention(row), axis=1)\n",
        "\n",
        "# Encode contents to feed to the model\n",
        "interesting_df[\"input_ids\"] = interesting_df[\"content\"].astype('str').apply(lambda text: tokenize(text))\n",
        "interesting_df[\"attention_words\"] = attention_df[\"attention_words\"]\n",
        "interesting_df[\"attention_mask\"] = attention_df[\"attention_mask\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPCgmLNiySVH"
      },
      "source": [
        "# Creating Prompt-Completion Pairs\n",
        "Now that we have the data, we need to put it in a form which can be used by the fine-tuning API. \n",
        "\n",
        "There is a token limit which you can find more about on these links: \n",
        "- https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them \n",
        "- https://platform.openai.com/docs/guides/fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euC198zvySVH",
        "outputId": "cc00c061-0418-4313-91fd-b5079441de6a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>completion</th>\n",
              "      <th>prompt_attention_mask</th>\n",
              "      <th>completion_attention_mask</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[16594, 257, 2272, 1705, 2708, 5115, 262, 1708...</td>\n",
              "      <td>[65, 6, 929, 19608, 642, 2231, 9114, 10183, 68...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[16594, 257, 2272, 1705, 2708, 5115, 262, 1708...</td>\n",
              "      <td>[65, 1053, 4908, 18828, 1891, 21883, 33779, 43...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[16594, 257, 2272, 1705, 2708, 5115, 262, 1708...</td>\n",
              "      <td>[65, 6, 86, 2542, 299, 15462, 16807, 1093, 82,...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[16594, 257, 2272, 1705, 2708, 5115, 262, 1708...</td>\n",
              "      <td>[65, 6, 86, 2542, 2429, 45610, 427, 707, 1207,...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[16594, 257, 2272, 1705, 2708, 5115, 262, 1708...</td>\n",
              "      <td>[65, 470, 13299, 781, 64, 2272, 87, 899, 312, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18349</th>\n",
              "      <td>[16594, 257, 2272, 1705, 2708, 5115, 262, 1708...</td>\n",
              "      <td>[65, 6, 8310, 962, 479, 437, 282, 28145, 1840,...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18350</th>\n",
              "      <td>[16594, 257, 2272, 1705, 2708, 5115, 262, 1708...</td>\n",
              "      <td>[65, 6, 929, 19608, 730, 65, 8949, 23, 9114, 1...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18351</th>\n",
              "      <td>[16594, 257, 2272, 1705, 2708, 5115, 262, 1708...</td>\n",
              "      <td>[65, 6, 86, 2542, 299, 15462, 1205, 890, 4354,...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18352</th>\n",
              "      <td>[16594, 257, 2272, 1705, 2708, 5115, 262, 1708...</td>\n",
              "      <td>[65, 6, 86, 2542, 4341, 3821, 1208, 474, 1726,...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18353</th>\n",
              "      <td>[16594, 257, 2272, 1705, 2708, 5115, 262, 1708...</td>\n",
              "      <td>[65, 6, 385, 1128, 865, 14029, 301, 259, 686, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18354 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  prompt  \\\n",
              "0      [16594, 257, 2272, 1705, 2708, 5115, 262, 1708...   \n",
              "1      [16594, 257, 2272, 1705, 2708, 5115, 262, 1708...   \n",
              "2      [16594, 257, 2272, 1705, 2708, 5115, 262, 1708...   \n",
              "3      [16594, 257, 2272, 1705, 2708, 5115, 262, 1708...   \n",
              "4      [16594, 257, 2272, 1705, 2708, 5115, 262, 1708...   \n",
              "...                                                  ...   \n",
              "18349  [16594, 257, 2272, 1705, 2708, 5115, 262, 1708...   \n",
              "18350  [16594, 257, 2272, 1705, 2708, 5115, 262, 1708...   \n",
              "18351  [16594, 257, 2272, 1705, 2708, 5115, 262, 1708...   \n",
              "18352  [16594, 257, 2272, 1705, 2708, 5115, 262, 1708...   \n",
              "18353  [16594, 257, 2272, 1705, 2708, 5115, 262, 1708...   \n",
              "\n",
              "                                              completion  \\\n",
              "0      [65, 6, 929, 19608, 642, 2231, 9114, 10183, 68...   \n",
              "1      [65, 1053, 4908, 18828, 1891, 21883, 33779, 43...   \n",
              "2      [65, 6, 86, 2542, 299, 15462, 16807, 1093, 82,...   \n",
              "3      [65, 6, 86, 2542, 2429, 45610, 427, 707, 1207,...   \n",
              "4      [65, 470, 13299, 781, 64, 2272, 87, 899, 312, ...   \n",
              "...                                                  ...   \n",
              "18349  [65, 6, 8310, 962, 479, 437, 282, 28145, 1840,...   \n",
              "18350  [65, 6, 929, 19608, 730, 65, 8949, 23, 9114, 1...   \n",
              "18351  [65, 6, 86, 2542, 299, 15462, 1205, 890, 4354,...   \n",
              "18352  [65, 6, 86, 2542, 4341, 3821, 1208, 474, 1726,...   \n",
              "18353  [65, 6, 385, 1128, 865, 14029, 301, 259, 686, ...   \n",
              "\n",
              "                                   prompt_attention_mask  \\\n",
              "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "1      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "2      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "4      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "...                                                  ...   \n",
              "18349  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "18350  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "18351  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "18352  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "18353  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "\n",
              "                               completion_attention_mask  \n",
              "0      [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
              "1      [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
              "2      [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, ...  \n",
              "3      [0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...  \n",
              "4      [0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, ...  \n",
              "...                                                  ...  \n",
              "18349  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, ...  \n",
              "18350  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, ...  \n",
              "18351  [0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
              "18352  [0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...  \n",
              "18353  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "\n",
              "[18354 rows x 4 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preprocess_prompt(prompt): \n",
        "    # Helper method to preprocess the prompt we want to create\n",
        "    prompt_tokens = tokenize(prompt)\n",
        "    return prompt_tokens\n",
        "\n",
        "def get_prompt(row):\n",
        "    # In this example, we will use a simple prompt: \n",
        "    topics = \", \".join(row.attention_words)\n",
        "    prompt = f\"Write a space news article regarding the following topics: {topics}\\n\\n###\\n\\n\"\n",
        "    preprocessed_prompt = preprocess_prompt(prompt)\n",
        "    return preprocessed_prompt\n",
        "\n",
        "def get_completion(row):\n",
        "    return row.input_ids\n",
        "\n",
        "def get_prompt_attention_mask(prompt):\n",
        "    return [1] * len(prompt)\n",
        "\n",
        "# Create the finetuning dataframe\n",
        "finetune_df = pd.DataFrame(columns=[\"prompt\", \"completion\"])\n",
        "finetune_df[\"prompt\"] = interesting_df.apply(lambda row: get_prompt(row), axis=1)\n",
        "finetune_df[\"completion\"] = interesting_df.apply(lambda row: get_completion(row), axis=1)\n",
        "# Create prompt and completion attention masks:\n",
        "finetune_df[\"prompt_attention_mask\"] = finetune_df[\"prompt\"].apply(lambda prompt: get_prompt_attention_mask(prompt))\n",
        "finetune_df[\"completion_attention_mask\"] = interesting_df[\"attention_mask\"]\n",
        "finetune_df\n",
        "# We now have data which we can feed to the model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLNkRI2TySVH"
      },
      "source": [
        "# Attempting to stay in line with Token Limits :S\n",
        "\n",
        "Generally, we have a 2048 token limit which applies to both the prompt and completion pair. In this case, we can drop those rows which go over the limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLqXSjI_ySVI",
        "outputId": "d144f3d5-55ab-404d-9d8f-5dee8edaaaab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         714.0\n",
              "1         454.0\n",
              "2         639.0\n",
              "3         569.0\n",
              "4         533.0\n",
              "          ...  \n",
              "18349     246.0\n",
              "18350     687.0\n",
              "18351     486.0\n",
              "18352     954.0\n",
              "18353    1062.0\n",
              "Name: token_lengths, Length: 18329, dtype: float64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_prompt_completion_lengths(row):\n",
        "    return len(row.prompt) + len(row.completion)\n",
        "\n",
        "def drop_long_lengths(row):\n",
        "    max_length = 2048\n",
        "    length = get_prompt_completion_lengths(row)\n",
        "    if length > max_length:\n",
        "        return None\n",
        "    return length\n",
        "\n",
        "finetune_df[\"token_lengths\"] = finetune_df.apply(lambda row: drop_long_lengths(row), axis=1)\n",
        "finetune_df = finetune_df[\"token_lengths\"].dropna(axis=0)\n",
        "finetune_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kGZepR-ySVI"
      },
      "source": [
        "From this, we now have data which we can feed into the model itself, and then prompt our fine-tuned model to generate an article for us! OpenAI has a detailed page on fine-tuning using multiple languages: https://platform.openai.com/docs/guides/fine-tuning"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "6d9d878d98508b2c7ab2062cb92b7f3624f5a76a988b1656f7e3e5969d8f7b2e"
    },
    "kernelspec": {
      "display_name": "Python 3.9.16 64-bit ('hackcambridge': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}